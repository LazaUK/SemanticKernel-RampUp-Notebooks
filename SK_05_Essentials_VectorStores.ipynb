{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d00088d4",
      "metadata": {},
      "source": [
        "## Semantic Kernel: Ramp-Up based on SK's Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c58d481c",
      "metadata": {},
      "source": [
        "To get the latest version of SK and PyPDF Python packages, use:\n",
        "\n",
        "``` bash\n",
        "pip install --upgrade semantic-kernel pypdf2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6db6c3f",
      "metadata": {},
      "source": [
        "## ðŸ“’ Notebook 5: Vector Store\n",
        "\n",
        "This notebook uses SK's In-Memory connector to create In-Memory Vector Store with PDF Documents\n",
        "\n",
        "**Updated for Semantic Kernel June 2025 Vector Store Migration**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645e4fe7",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 1: Configure environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "44ba3684",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import os\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Annotated\n",
        "from uuid import uuid4\n",
        "from pathlib import Path\n",
        "import traceback\n",
        "\n",
        "# PDF processing\n",
        "import PyPDF2\n",
        "\n",
        "# Semantic Kernel imports\n",
        "from semantic_kernel import Kernel\n",
        "from semantic_kernel.contents import ChatHistory\n",
        "from semantic_kernel.connectors.ai.open_ai import (\n",
        "    AzureChatCompletion,\n",
        "    AzureTextEmbedding,\n",
        "    OpenAIChatPromptExecutionSettings\n",
        ")\n",
        "\n",
        "# NEW: Vector store imports for updated API\n",
        "from semantic_kernel.connectors.in_memory import InMemoryCollection\n",
        "from semantic_kernel.data.vector import (\n",
        "    VectorStoreField,\n",
        "    vectorstoremodel,\n",
        "    DistanceFunction,\n",
        "    IndexKind\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "85c91018",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set Azure OpenAI backend variables\n",
        "AOAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_API_DEPLOY\")\n",
        "AOAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
        "AOAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "AOAI_EMBEDDING = os.getenv(\"AZURE_OPENAI_API_DEPLOY_EMBED\")\n",
        "\n",
        "# Set data folder path\n",
        "DATA_FOLDER = \"data\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a3f0f2e",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 2: Define Data Model and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "data_model_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set constants for vector search\n",
        "DISTANCE_FUNCTION = DistanceFunction.COSINE_SIMILARITY\n",
        "INDEX_KIND = IndexKind.HNSW\n",
        "\n",
        "# Class for vector store's data model\n",
        "@vectorstoremodel\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    \"\"\"Data model for document chunks with vector embeddings.\"\"\"\n",
        "    content: Annotated[\n",
        "        str,\n",
        "        VectorStoreField(\"data\", is_full_text_indexed=True),\n",
        "    ] = \"content\"\n",
        "    id: Annotated[str, VectorStoreField(\"key\")] = field(default_factory=lambda: str(uuid4()))\n",
        "    vector: Annotated[\n",
        "        list[float] | str | None,\n",
        "        VectorStoreField(\n",
        "            \"vector\",\n",
        "            dimensions = 1536,\n",
        "            index_kind = INDEX_KIND,\n",
        "            distance_function = DISTANCE_FUNCTION,\n",
        "        ),\n",
        "    ] = None\n",
        "    document_name: Annotated[str, VectorStoreField(\"data\", is_indexed=True)] = \"document\"\n",
        "    page_number: Annotated[int, VectorStoreField(\"data\", is_indexed=True)] = 0\n",
        "    chunk_index: Annotated[int, VectorStoreField(\"data\", is_indexed=True)] = 0\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        # Set vector to content if not provided - embedder will generate it\n",
        "        if self.vector is None:\n",
        "            self.vector = self.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "helper_functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to extract text from PDF files\n",
        "def extract_text_from_pdf(pdf_path: Path) -> list[tuple[str, int]]:\n",
        "    \"\"\"Extract text from PDF file, returning list of (text, page_number) tuples.\"\"\"\n",
        "    pages_text = []\n",
        "    \n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
        "                text = page.extract_text()\n",
        "                if text.strip():\n",
        "                    pages_text.append((text.strip(), page_num))\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
        "    \n",
        "    return pages_text\n",
        "\n",
        "# Helper function to split text into smaller pieces\n",
        "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> list[str]:\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        \n",
        "        if end < len(text):\n",
        "            last_period = chunk.rfind('.')\n",
        "            if last_period > chunk_size // 2:\n",
        "                chunk = chunk[:last_period + 1]\n",
        "                end = start + len(chunk)\n",
        "        \n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "        \n",
        "        if end >= len(text):\n",
        "            break\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Helper function to process all PDF files in a folder\n",
        "def process_pdfs_from_folder(folder_path: str) -> list[DocumentChunk]:\n",
        "    \"\"\"Process all PDF files in the specified folder and return DocumentChunk objects.\"\"\"\n",
        "    data_path = Path(folder_path)\n",
        "    \n",
        "    if not data_path.exists():\n",
        "        print(f\"Warning: Data folder '{folder_path}' does not exist.\")\n",
        "        return []\n",
        "    \n",
        "    document_chunks = []\n",
        "    pdf_files = list(data_path.glob(\"*.pdf\"))\n",
        "    \n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in '{folder_path}'.\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"Found {len(pdf_files)} PDF files to process:\")\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"  - {pdf_file.name}\")\n",
        "    \n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "        pages_text = extract_text_from_pdf(pdf_file)\n",
        "        \n",
        "        for page_text, page_num in pages_text:\n",
        "            chunks = chunk_text(page_text)\n",
        "            \n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                document_chunk = DocumentChunk(\n",
        "                    content = chunk,\n",
        "                    document_name = pdf_file.stem,\n",
        "                    page_number = page_num,\n",
        "                    chunk_index = chunk_idx\n",
        "                )\n",
        "                document_chunks.append(document_chunk)\n",
        "    \n",
        "    print(f\"\\nTotal chunks created: {len(document_chunks)}\")\n",
        "    return document_chunks\n",
        "\n",
        "# Helper function to print search results\n",
        "def print_search_result(result, score: float = None):\n",
        "    \"\"\"Print a search result in a formatted way.\"\"\"\n",
        "    print(f\"Document: {result.document_name}\")\n",
        "    print(f\"Page: {result.page_number}, Chunk: {result.chunk_index}\")\n",
        "    if score is not None:\n",
        "        print(f\"Relevance Score: {score:.4f}\")\n",
        "    print(f\"Content: {result.content[:200]}{'...' if len(result.content) > 200 else ''}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "043f1baa",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 3: Initialise Kernel and Services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67498941",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialise kernel\n",
        "kernel = Kernel()\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3f23ef98",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure OpenAI embedding service added\n"
          ]
        }
      ],
      "source": [
        "# Add Azure OpenAI embedding\n",
        "if AOAI_EMBEDDING and AOAI_ENDPOINT and AOAI_API_VERSION:\n",
        "    embedder = AzureTextEmbedding(\n",
        "        deployment_name = AOAI_EMBEDDING,\n",
        "        endpoint = AOAI_ENDPOINT,\n",
        "        api_version = AOAI_API_VERSION,\n",
        "        service_id = \"embedding\"\n",
        "    )\n",
        "    kernel.add_service(embedder)\n",
        "    print(\"Azure OpenAI embedding service added\")\n",
        "else:\n",
        "    print(\"Azure OpenAI embedding not configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0a59c9ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Azure OpenAI chat completion service added\n"
          ]
        }
      ],
      "source": [
        "# Add Azure OpenAI chat completion\n",
        "if AOAI_DEPLOYMENT and AOAI_ENDPOINT and AOAI_API_VERSION:\n",
        "    chat_completion = AzureChatCompletion(\n",
        "        deployment_name = AOAI_DEPLOYMENT,\n",
        "        endpoint = AOAI_ENDPOINT,\n",
        "        api_version = AOAI_API_VERSION,\n",
        "        service_id = \"azure_openai_chat\",\n",
        "    )\n",
        "    kernel.add_service(chat_completion)\n",
        "    print(\"Azure OpenAI chat completion service added\")\n",
        "else:\n",
        "    print(\"Azure OpenAI chat completion not configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pdf_processing",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 4: Load and Process PDF Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "load_pdfs",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing PDF documents from data folder...\n",
            "Found 2 PDF files to process:\n",
            "  - NorthwindHealthPlus_BenefitsDetails.pdf\n",
            "  - Northwind_Standard_Benefits_Details.pdf\n",
            "\n",
            "Processing: NorthwindHealthPlus_BenefitsDetails.pdf\n",
            "\n",
            "Processing: Northwind_Standard_Benefits_Details.pdf\n",
            "\n",
            "Total chunks created: 684\n",
            "Successfully processed 684 document chunks\n",
            "\n",
            "Document Summary:\n",
            "- NorthwindHealthPlus_BenefitsDetails: 109 pages, 344 chunks\n",
            "- Northwind_Standard_Benefits_Details: 104 pages, 340 chunks\n"
          ]
        }
      ],
      "source": [
        "# Process PDF documents from the data folder\n",
        "print(\"Processing PDF documents from data folder...\")\n",
        "document_chunks = process_pdfs_from_folder(DATA_FOLDER)\n",
        "\n",
        "if not document_chunks:\n",
        "    print(\"No documents to process. Please ensure PDF files are in the 'data' folder.\")\n",
        "else:\n",
        "    print(f\"Successfully processed {len(document_chunks)} document chunks\")\n",
        "    \n",
        "    # Display summary\n",
        "    doc_summary = {}\n",
        "    for chunk in document_chunks:\n",
        "        if chunk.document_name not in doc_summary:\n",
        "            doc_summary[chunk.document_name] = {'pages': set(), 'chunks': 0}\n",
        "        doc_summary[chunk.document_name]['pages'].add(chunk.page_number)\n",
        "        doc_summary[chunk.document_name]['chunks'] += 1\n",
        "    \n",
        "    print(\"\\nDocument Summary:\")\n",
        "    for doc_name, info in doc_summary.items():\n",
        "        print(f\"- {doc_name}: {len(info['pages'])} pages, {info['chunks']} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vector_store",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 5: Create Vector Store and Upsert Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "create_vector_store",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating vector store collection...\n",
            "Collection created\n",
            "Upserting records to vector store (embeddings generated automatically)...\n",
            "Upserted 684 records to vector store\n"
          ]
        }
      ],
      "source": [
        "# Create and populate the vector store\n",
        "async def create_and_populate_vector_store():\n",
        "    if not document_chunks:\n",
        "        print(\"No document chunks to process\")\n",
        "        return None\n",
        "    \n",
        "    print(\"Creating vector store collection...\")\n",
        "    \n",
        "    # Define collection with embedding_generator parameter\n",
        "    record_collection = InMemoryCollection[str, DocumentChunk](\n",
        "        collection_name = \"pdf_documents\",\n",
        "        record_type = DocumentChunk,\n",
        "        embedding_generator = embedder\n",
        "    )\n",
        "    \n",
        "    async with record_collection:\n",
        "        # Create the collection\n",
        "        await record_collection.ensure_collection_exists()\n",
        "        print(\"Collection created\")\n",
        "        \n",
        "        # Upsert the document chunks\n",
        "        print(\"Upserting records to vector store (embeddings generated automatically)...\")\n",
        "        keys = await record_collection.upsert(document_chunks)\n",
        "        print(f\"Upserted {len(keys)} records to vector store\")\n",
        "        \n",
        "        return record_collection\n",
        "\n",
        "# Run the async function\n",
        "vector_collection = await create_and_populate_vector_store()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "search_demo",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 6: Search Demos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "19c9b72a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search functions ready.\n"
          ]
        }
      ],
      "source": [
        "# Search documents in the vector store\n",
        "async def search_documents(query: str, document_filter: str = None, top_k: int = 5):\n",
        "    \"\"\"Search for documents based on a query, prints results and returns them.\"\"\"\n",
        "\n",
        "    print(f\"Searching for: '{query}'\")\n",
        "    if document_filter:\n",
        "        print(f\"Filtering by document: {document_filter}\")\n",
        "\n",
        "    retrieved_data = []\n",
        "\n",
        "    try:\n",
        "        # Set up search options as dict\n",
        "        options = {\n",
        "            \"vector_property_name\": \"vector\",\n",
        "            \"top\": top_k,\n",
        "        }\n",
        "        \n",
        "        # Add filter if specified\n",
        "        if document_filter:\n",
        "            options[\"filter\"] = lambda x: x.document_name == document_filter\n",
        "        \n",
        "        # Use search method with query string - it generates embedding automatically\n",
        "        search_results = await vector_collection.search(\n",
        "            values=query,\n",
        "            **options,\n",
        "        )\n",
        "        \n",
        "        if search_results.total_count == 0:\n",
        "            print(\"No results found\")\n",
        "            return []\n",
        "        \n",
        "        print(f\"\\nFound {search_results.total_count} matching results (showing top {top_k}):\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        async for result in search_results.results:\n",
        "            retrieved_data.append((result.record, result.score))\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Search error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "        \n",
        "    return retrieved_data\n",
        "\n",
        "print(\"Search functions ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "search_example1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for: 'Tips for Employees'\n",
            "\n",
            "Found None matching results (showing top 3):\n",
            "================================================================================\n",
            "Document: Northwind_Standard_Benefits_Details\n",
            "Page: 92, Chunk: 0\n",
            "Relevance Score: 0.5155\n",
            "Content: 2. Ask questions. If the employee is unsure about any part of the plan, it is important to ask \n",
            "questions in order to make sure that the plan is suitable for their needs.  \n",
            "3. Research other plans. It...\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Northwind_Standard_Benefits_Details\n",
            "Page: 90, Chunk: 2\n",
            "Relevance Score: 0.4785\n",
            "Content: important for employees to make sure they are familiar with \n",
            "the providerâ€™s policies and procedures. Employees should also make sure they und erstand \n",
            "any additional costs that may be associated with ...\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Northwind_Standard_Benefits_Details\n",
            "Page: 86, Chunk: 2\n",
            "Relevance Score: 0.4761\n",
            "Content: and responsibilities under the \n",
            "law when it comes to their employer -provided health insurance plan. Here are a few tips \n",
            "employees should keep in mind:  \n",
            "â€¢ Be aware of the terms of your h ealth plan:...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example 1: General search\n",
        "results = await search_documents(\"Tips for Employees\", top_k=3)\n",
        "for record, score in results:\n",
        "    print_search_result(record, score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6119ad4f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for: 'What is covered by the Northwind Health Standard Plan?'\n",
            "Filtering by document: Northwind_Standard_Benefits_Details\n",
            "\n",
            "Found None matching results (showing top 3):\n",
            "================================================================================\n",
            "Document: Northwind_Standard_Benefits_Details\n",
            "Page: 93, Chunk: 2\n",
            "Relevance Score: 0.7815\n",
            "Content: ant to note that while Northwind Standard covers a variety of services, there are \n",
            "some except ions. These include emergency services, mental health and substance abuse \n",
            "coverage, and out -of-network ...\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Northwind_Standard_Benefits_Details\n",
            "Page: 3, Chunk: 0\n",
            "Relevance Score: 0.7749\n",
            "Content: Summary of Benefits  \n",
            "Northwind Standard  \n",
            "Northwind Standard is a basic plan that provides coverage for medical, vision, a nd dental \n",
            "services. This plan also offers coverage for preventive care serv...\n",
            "--------------------------------------------------------------------------------\n",
            "Document: Northwind_Standard_Benefits_Details\n",
            "Page: 99, Chunk: 0\n",
            "Relevance Score: 0.7699\n",
            "Content: Under this plan, Northwind Health will cover the cost of eligible services you receive, as \n",
            "long as you follow certain rules. Some se rvices may require pre -authorization or be subject \n",
            "to an annual ...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Search within a specific document\n",
        "results = await search_documents(\n",
        "    query = \"What is covered by the Northwind Health Standard Plan?\",\n",
        "    document_filter = \"Northwind_Standard_Benefits_Details\",\n",
        "    top_k = 3\n",
        ")\n",
        "for record, score in results:\n",
        "    print_search_result(record, score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c949b55d",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 7: RAG with PDF Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a09b45ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG function is ready.\n"
          ]
        }
      ],
      "source": [
        "# Perform a Retrieval Augmented Generation (RAG) query\n",
        "async def perform_rag_query(\n",
        "    query: str,\n",
        "    document_filter: str = None,\n",
        "    top_k_retrieval: int = 3,\n",
        "    chat_service_id: str = \"azure_openai_chat\" \n",
        "):\n",
        "    \"\"\"\n",
        "    Performs Retrieval Augmented Generation:\n",
        "    1. Uses search_documents to retrieve relevant document chunks.\n",
        "    2. Constructs a ChatHistory object with system and user messages.\n",
        "    3. Calls the chat service's get_chat_message_contents method.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        chat_completion_service = kernel.get_service(chat_service_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting chat service '{chat_service_id}' for RAG: {e}\")\n",
        "        return\n",
        "\n",
        "    # Retrieve relevant document chunks\n",
        "    search_results_tuples = await search_documents(\n",
        "        query = query,\n",
        "        document_filter = document_filter,\n",
        "        top_k = top_k_retrieval\n",
        "    )\n",
        "\n",
        "    retrieved_chunks_content = []\n",
        "    if not search_results_tuples:\n",
        "        print(\"No document chunks retrieved to use for context.\")\n",
        "    else:\n",
        "        for record, score in search_results_tuples:\n",
        "            retrieved_chunks_content.append(record.content)\n",
        "\n",
        "    # Construct ChatHistory\n",
        "    chat_history = ChatHistory()\n",
        "    system_message = (\n",
        "        \"You are a helpful AI assistant. You are provided with context from documents. \"\n",
        "        \"Your task is to answer the user's question based ONLY on this provided context. \"\n",
        "        \"Do not use any external knowledge or make assumptions beyond what is stated in the context. \"\n",
        "        \"If the information to answer the question is not present in the context, \"\n",
        "        \"clearly state that you cannot answer based on the provided information. \"\n",
        "        \"Be concise and directly answer the question.\"\n",
        "    )\n",
        "    chat_history.add_system_message(system_message)\n",
        "\n",
        "    if retrieved_chunks_content:\n",
        "        context_str = \"\\n\\n---\\n\\n\".join(retrieved_chunks_content)\n",
        "        user_message_text = f\"\"\"Here is the context from the documents:\n",
        "<context>\n",
        "{context_str}\n",
        "</context>\n",
        "\n",
        "Based ONLY on the context provided above, please answer the following question.\n",
        "User Question: {query}\"\"\"\n",
        "    else:\n",
        "        user_message_text = f\"\"\"User Question: {query}\n",
        "\n",
        "(System note: No specific context was retrieved for this question from the documents. \n",
        "Based on your instructions, if the answer is not in the documents, please state that.)\"\"\"\n",
        "\n",
        "    chat_history.add_user_message(user_message_text)\n",
        "\n",
        "    # Generate answer using chat service\n",
        "    try:\n",
        "        execution_settings = OpenAIChatPromptExecutionSettings(\n",
        "            service_id=chat_service_id,\n",
        "        )\n",
        "        \n",
        "        response_messages = await chat_completion_service.get_chat_message_contents(\n",
        "            chat_history = chat_history,\n",
        "            settings = execution_settings\n",
        "        )\n",
        "\n",
        "        print(\"\\nLLM Response:\")\n",
        "        if response_messages and isinstance(response_messages, list) and len(response_messages) > 0:\n",
        "            assistant_message_content = response_messages[0]\n",
        "            if hasattr(assistant_message_content, 'content') and assistant_message_content.content is not None:\n",
        "                print(str(assistant_message_content.content))\n",
        "            elif hasattr(assistant_message_content, 'items') and assistant_message_content.items and \\\n",
        "                 hasattr(assistant_message_content.items[0], 'text'):\n",
        "                print(assistant_message_content.items[0].text)\n",
        "            else: \n",
        "                print(str(assistant_message_content))\n",
        "        else:\n",
        "            print(\"\\nLLM returned an empty or unexpected response format.\")\n",
        "            print(f\"Raw response: {response_messages}\")\n",
        "\n",
        "    except AttributeError as e:\n",
        "        print(f\"AttributeError during LLM invocation: {e}\")\n",
        "        traceback.print_exc()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM invocation: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"RAG function is ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2423d0d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for: 'What are the key benefits of the Northwind Health Plus plan?'\n",
            "\n",
            "Found None matching results (showing top 3):\n",
            "================================================================================\n",
            "\n",
            "LLM Response:\n",
            "The key benefits of the Northwind Health Plus plan include comprehensive coverage for medical, vision, and dental services; prescription drugs; mental health and substance abuse services; and preventive care. It offers access to a variety of in-network providers such as primary care physicians, specialists, hospitals, and pharmacies. The plan also covers emergency services both in-network and out-of-network. Additionally, it is a group health plan sponsored by Contoso, with shared premium payments between the employer and employee.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# General RAG query across all documents\n",
        "await perform_rag_query(\n",
        "    query = \"What are the key benefits of the Northwind Health Plus plan?\",\n",
        "    top_k_retrieval = 3\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 8: Cleanup (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cleanup_cell",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector collection cleaned up\n"
          ]
        }
      ],
      "source": [
        "# Clean up the vector collection\n",
        "async def cleanup_collection():\n",
        "    if vector_collection:\n",
        "        try:\n",
        "            async with vector_collection:\n",
        "                await vector_collection.ensure_collection_deleted()\n",
        "            print(\"Vector collection cleaned up\")\n",
        "        except Exception as e:\n",
        "            print(f\"Cleanup error: {e}\")\n",
        "    else:\n",
        "        print(\"No collection to clean up\")\n",
        "\n",
        "await cleanup_collection()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
