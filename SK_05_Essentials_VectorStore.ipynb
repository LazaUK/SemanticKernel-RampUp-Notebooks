{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d00088d4",
      "metadata": {},
      "source": [
        "## Semantic Kernel: Ramp-Up based on SK's Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c58d481c",
      "metadata": {},
      "source": [
        "To get the latest version of SK and PyPDF Python packages, use:\n",
        "\n",
        "``` bash\n",
        "pip install --upgrade semantic-kernel pypdf2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6db6c3f",
      "metadata": {},
      "source": [
        "## ðŸ“’ Notebook 5: Vector Store\n",
        "\n",
        "This notebook uses SK's In-Memory connector to create In-Memory Vector Store with PDF Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645e4fe7",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 1: Configure environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44ba3684",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import os\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Annotated\n",
        "from uuid import uuid4\n",
        "from pathlib import Path\n",
        "import traceback; \n",
        "\n",
        "# PDF processing\n",
        "import PyPDF2\n",
        "\n",
        "# Semantic Kernel imports\n",
        "from semantic_kernel import Kernel\n",
        "from semantic_kernel.contents import ChatHistory\n",
        "from semantic_kernel.connectors.ai.open_ai import (\n",
        "    AzureChatCompletion,\n",
        "    AzureTextEmbedding,\n",
        "    OpenAIEmbeddingPromptExecutionSettings,\n",
        "    OpenAIChatPromptExecutionSettings\n",
        ")\n",
        "\n",
        "# Memory and vector store imports\n",
        "from semantic_kernel.connectors.memory.in_memory import InMemoryVectorCollection\n",
        "from semantic_kernel.data import (\n",
        "    VectorSearchFilter,\n",
        "    VectorSearchOptions,\n",
        "    VectorStoreRecordDataField,\n",
        "    VectorStoreRecordKeyField,\n",
        "    VectorStoreRecordVectorField,\n",
        "    vectorstoremodel,\n",
        ")\n",
        "from semantic_kernel.data.const import DISTANCE_FUNCTION_DIRECTION_HELPER, DistanceFunction, IndexKind\n",
        "from semantic_kernel.data.vector_search import add_vector_to_records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85c91018",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set Azure OpenAI backend variables\n",
        "AOAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_API_DEPLOY\")\n",
        "AOAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
        "AOAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "AOAI_EMBEDDING = os.getenv(\"AZURE_OPENAI_API_DEPLOY_EMBED\")\n",
        "\n",
        "# Set data folder path\n",
        "DATA_FOLDER = \"data\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a3f0f2e",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 2: Define Data Model and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "data_model_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set constants for vector search\n",
        "DISTANCE_FUNCTION = DistanceFunction.COSINE_SIMILARITY\n",
        "INDEX_KIND = IndexKind.IVF_FLAT\n",
        "\n",
        "# Class for vector store's data model\n",
        "@vectorstoremodel\n",
        "@dataclass\n",
        "class DocumentChunk:\n",
        "    \"\"\"Data model for document chunks with vector embeddings.\"\"\"\n",
        "    vector: Annotated[\n",
        "        list[float] | None,\n",
        "        VectorStoreRecordVectorField(\n",
        "            embedding_settings = {\"embedding\": OpenAIEmbeddingPromptExecutionSettings()},\n",
        "            index_kind = INDEX_KIND,\n",
        "            dimensions = 1536,\n",
        "            distance_function = DISTANCE_FUNCTION,\n",
        "            property_type = \"float\",\n",
        "        ),\n",
        "    ] = None\n",
        "    id: Annotated[str, VectorStoreRecordKeyField()] = field(default_factory=lambda: str(uuid4()))\n",
        "    content: Annotated[\n",
        "        str,\n",
        "        VectorStoreRecordDataField(\n",
        "            has_embedding = True,\n",
        "            embedding_property_name = \"vector\",\n",
        "            property_type = \"str\",\n",
        "            is_full_text_searchable = True,\n",
        "        ),\n",
        "    ] = \"content\"\n",
        "    document_name: Annotated[str, VectorStoreRecordDataField(property_type=\"str\", is_filterable=True)] = \"document\"\n",
        "    page_number: Annotated[int, VectorStoreRecordDataField(property_type=\"int\", is_filterable=True)] = 0\n",
        "    chunk_index: Annotated[int, VectorStoreRecordDataField(property_type=\"int\", is_filterable=True)] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "helper_functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to extract text from PDF files\n",
        "def extract_text_from_pdf(pdf_path: Path) -> list[tuple[str, int]]:\n",
        "    \"\"\"Extract text from PDF file, returning list of (text, page_number) tuples.\"\"\"\n",
        "    pages_text = []\n",
        "    \n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
        "                text = page.extract_text()\n",
        "                if text.strip():  # Only add non-empty pages\n",
        "                    pages_text.append((text.strip(), page_num))\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
        "    \n",
        "    return pages_text\n",
        "\n",
        "# Helper function to split text into smaller pieces\n",
        "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> list[str]:\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    if len(text) <= chunk_size:\n",
        "        return [text]\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        \n",
        "        # Try to break at sentence boundary if possible\n",
        "        if end < len(text):\n",
        "            last_period = chunk.rfind('.')\n",
        "            if last_period > chunk_size // 2:  # Only break if it's not too early\n",
        "                chunk = chunk[:last_period + 1]\n",
        "                end = start + len(chunk)\n",
        "        \n",
        "        chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "        \n",
        "        if end >= len(text):\n",
        "            break\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "# Helper function to process all PDF files in a folder and return DocumentChunk objects\n",
        "def process_pdfs_from_folder(folder_path: str) -> list[DocumentChunk]:\n",
        "    \"\"\"Process all PDF files in the specified folder and return DocumentChunk objects.\"\"\"\n",
        "    data_path = Path(folder_path)\n",
        "    \n",
        "    if not data_path.exists():\n",
        "        print(f\"Warning: Data folder '{folder_path}' does not exist.\")\n",
        "        return []\n",
        "    \n",
        "    document_chunks = []\n",
        "    pdf_files = list(data_path.glob(\"*.pdf\"))\n",
        "    \n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in '{folder_path}'.\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"Found {len(pdf_files)} PDF files to process:\")\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"  - {pdf_file.name}\")\n",
        "    \n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "        pages_text = extract_text_from_pdf(pdf_file)\n",
        "        \n",
        "        for page_text, page_num in pages_text:\n",
        "            chunks = chunk_text(page_text)\n",
        "            # print(f\"  Page {page_num}: {len(chunks)} chunks\")\n",
        "            \n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                document_chunk = DocumentChunk(\n",
        "                    content = chunk,\n",
        "                    document_name = pdf_file.stem,  # filename without extension\n",
        "                    page_number = page_num,\n",
        "                    chunk_index = chunk_idx\n",
        "                )\n",
        "                document_chunks.append(document_chunk)\n",
        "    \n",
        "    print(f\"\\nTotal chunks created: {len(document_chunks)}\")\n",
        "    return document_chunks\n",
        "\n",
        "# Helper function to print search results\n",
        "def print_search_result(result, score: float = None):\n",
        "    \"\"\"Print a search result in a formatted way.\"\"\"\n",
        "    print(f\"Document: {result.document_name}\")\n",
        "    print(f\"Page: {result.page_number}, Chunk: {result.chunk_index}\")\n",
        "    if score is not None:\n",
        "        print(f\"Relevance Score: {score:.4f}\")\n",
        "    print(f\"Content: {result.content[:200]}{'...' if len(result.content) > 200 else ''}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "043f1baa",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 3: Initialise Kernel and Services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67498941",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialise kernel\n",
        "kernel = Kernel()\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f23ef98",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add Azure OpenAI embedding\n",
        "if AOAI_EMBEDDING and AOAI_ENDPOINT and AOAI_API_VERSION:\n",
        "    embedder = AzureTextEmbedding(\n",
        "        deployment_name = AOAI_EMBEDDING,\n",
        "        endpoint = AOAI_ENDPOINT,\n",
        "        api_version = AOAI_API_VERSION,\n",
        "        service_id = \"embedding\"\n",
        "    )\n",
        "    kernel.add_service(embedder)\n",
        "    print(\"Azure OpenAI embedding service added\")\n",
        "else:\n",
        "    print(\"Azure OpenAI embedding not configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a59c9ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add Azure OpenAI chat completion\n",
        "if AOAI_DEPLOYMENT and AOAI_ENDPOINT and AOAI_API_VERSION:\n",
        "    chat_completion = AzureChatCompletion(\n",
        "        deployment_name = AOAI_DEPLOYMENT,\n",
        "        endpoint = AOAI_ENDPOINT,\n",
        "        api_version = AOAI_API_VERSION,\n",
        "        service_id = \"azure_openai_chat\",\n",
        "    )\n",
        "    kernel.add_service(chat_completion)\n",
        "    print(\"Azure OpenAI chat completion service added\")\n",
        "else:\n",
        "    print(\"Azure OpenAI chat completion not configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pdf_processing",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 4: Load and Process PDF Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load_pdfs",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process PDF documents from the data folder\n",
        "print(\"Processing PDF documents from data folder...\")\n",
        "document_chunks = process_pdfs_from_folder(DATA_FOLDER)\n",
        "\n",
        "if not document_chunks:\n",
        "    print(\"No documents to process. Please ensure PDF files are in the 'data' folder.\")\n",
        "else:\n",
        "    print(f\"Successfully processed {len(document_chunks)} document chunks\")\n",
        "    \n",
        "    # Display summary\n",
        "    doc_summary = {}\n",
        "    for chunk in document_chunks:\n",
        "        if chunk.document_name not in doc_summary:\n",
        "            doc_summary[chunk.document_name] = {'pages': set(), 'chunks': 0}\n",
        "        doc_summary[chunk.document_name]['pages'].add(chunk.page_number)\n",
        "        doc_summary[chunk.document_name]['chunks'] += 1\n",
        "    \n",
        "    print(\"\\nDocument Summary:\")\n",
        "    for doc_name, info in doc_summary.items():\n",
        "        print(f\"- {doc_name}: {len(info['pages'])} pages, {info['chunks']} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vector_store",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 5: Create Vector Store and Upsert Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create_vector_store",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and populate the vector store\n",
        "async def create_and_populate_vector_store():\n",
        "    if not document_chunks:\n",
        "        print(\"No document chunks to process\")\n",
        "        return None\n",
        "    \n",
        "    print(\"Creating vector store collection...\")\n",
        "    \n",
        "    # Create the collection\n",
        "    record_collection = InMemoryVectorCollection[str, DocumentChunk](\n",
        "        collection_name = \"pdf_documents\",\n",
        "        data_model_type = DocumentChunk,\n",
        "    )\n",
        "    \n",
        "    async with record_collection:\n",
        "        # Create the collection after wiping it\n",
        "        await record_collection.delete_collection()\n",
        "        await record_collection.create_collection_if_not_exists()\n",
        "        print(\"Collection created\")\n",
        "        \n",
        "        # Generate embeddings and upsert records\n",
        "        print(\"Generating embeddings for document chunks...\")\n",
        "        records_with_embedding = await add_vector_to_records(\n",
        "            kernel, document_chunks, data_model_type=DocumentChunk\n",
        "        )\n",
        "        \n",
        "        print(\"Upserting records to vector store...\")\n",
        "        keys = await record_collection.upsert(records_with_embedding)\n",
        "        print(f\"Upserted {len(keys)} records to vector store\")\n",
        "        \n",
        "        return record_collection\n",
        "\n",
        "# Run the async function\n",
        "vector_collection = await create_and_populate_vector_store()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "search_demo",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 6: Search Demos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c9b72a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search documents in the vector store\n",
        "async def search_documents(query: str, document_filter: str = None, top_k: int = 5):\n",
        "    \"\"\"Search for documents based on a query, prints results and returns them.\"\"\"\n",
        "\n",
        "    print(f\"Searching for: '{query}'\")\n",
        "    if document_filter:\n",
        "        print(f\"Filtering by document: {document_filter}\")\n",
        "\n",
        "    retrieved_data = []\n",
        "\n",
        "    # Set up search options\n",
        "    if document_filter:\n",
        "        search_filter = VectorSearchFilter.equal_to(\"document_name\", document_filter)\n",
        "        options = VectorSearchOptions(\n",
        "            vector_field_name = \"vector\",\n",
        "            include_vectors = False,\n",
        "            filter = search_filter,\n",
        "            top = top_k\n",
        "        )\n",
        "    else:\n",
        "        options = VectorSearchOptions(\n",
        "            vector_field_name = \"vector\",\n",
        "            include_vectors = False,\n",
        "            top = top_k\n",
        "        )\n",
        "    \n",
        "    try:\n",
        "        # Generate embedding for the query\n",
        "        query_embedding = (await embedder.generate_raw_embeddings([query]))[0]\n",
        "        \n",
        "        # Perform the search\n",
        "        async with vector_collection:\n",
        "            search_results = await vector_collection.vectorized_search(\n",
        "                vector = query_embedding,\n",
        "                options = options,\n",
        "            )\n",
        "            \n",
        "            results_to_process = []\n",
        "            async for result_item in search_results.results:\n",
        "                results_to_process.append(result_item)\n",
        "\n",
        "            if not results_to_process:\n",
        "                print(\"No results found\")\n",
        "                return []\n",
        "            \n",
        "            print(f\"\\nFound {len(results_to_process)} matching results (max to use - {top_k}):\")\n",
        "            print(\"=\" * 80)\n",
        "            \n",
        "            for result in results_to_process:\n",
        "                # print_search_result(result.record, result.score)\n",
        "                retrieved_data.append((result.record, result.score))\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Search error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "        \n",
        "    return retrieved_data\n",
        "\n",
        "print(\"Search functions ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "search_example1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: General search\n",
        "await search_documents(\"Tips for Employees\", top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6119ad4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Search within a specific document\n",
        "await search_documents(\n",
        "    query = \"What is covered by the Northwind Health Standard Plan?\",\n",
        "    document_filter = \"Northwind_Standard_Benefits_Details\",\n",
        "    top_k = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c949b55d",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 7: RAG with PDF Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a09b45ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform a Retrieval Augmented Generation (RAG) query\n",
        "async def perform_rag_query(\n",
        "    query: str,\n",
        "    document_filter: str = None,\n",
        "    top_k_retrieval: int = 3,\n",
        "    chat_service_id: str = \"azure_openai_chat\" \n",
        "):\n",
        "    \"\"\"\n",
        "    Performs Retrieval Augmented Generation (Simplified):\n",
        "    1. Uses search_documents to retrieve relevant document chunks.\n",
        "    2. Constructs a ChatHistory object with a system and user messages.\n",
        "    3. Calls the chat service's get_chat_message_contents method.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        chat_completion_service = kernel.get_service(chat_service_id)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting chat service '{chat_service_id}' for RAG: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 1. Retrieve relevant document chunks using search_documents ---\n",
        "    search_results_tuples = await search_documents(\n",
        "        query = query,\n",
        "        document_filter = document_filter,\n",
        "        top_k = top_k_retrieval\n",
        "    )\n",
        "\n",
        "    retrieved_chunks_content = []\n",
        "    if not search_results_tuples:\n",
        "        print(\"No document chunks retrieved by search_documents to use for context.\")\n",
        "    else:\n",
        "        # print(f\"\\nUsing {len(search_results_tuples)} retrieved chunk(s) for context generation:\")\n",
        "        for record, score in search_results_tuples:\n",
        "            retrieved_chunks_content.append(record.content)\n",
        "            # print(f\"      - RAG Context: Doc: {record.document_name}, Page: {record.page_number}, Chunk: {record.chunk_index}, Score: {score:.4f} (Content snippet: '{record.content[:50].strip()}...')\")\n",
        "\n",
        "    # --- 2. Construct ChatHistory ---\n",
        "    chat_history = ChatHistory()\n",
        "    system_message = (\n",
        "        \"You are a helpful AI assistant. You are provided with context from documents. \"\n",
        "        \"Your task is to answer the user's question based ONLY on this provided context. \"\n",
        "        \"Do not use any external knowledge or make assumptions beyond what is stated in the context. \"\n",
        "        \"If the information to answer the question is not present in the context, \"\n",
        "        \"clearly state that you cannot answer based on the provided information. \"\n",
        "        \"Be concise and directly answer the question.\"\n",
        "    )\n",
        "    chat_history.add_system_message(system_message)\n",
        "\n",
        "    if retrieved_chunks_content:\n",
        "        context_str = \"\\n\\n---\\n\\n\".join(retrieved_chunks_content)\n",
        "        user_message_text = f\"\"\"Here is the context from the documents:\n",
        "<context>\n",
        "{context_str}\n",
        "</context>\n",
        "\n",
        "Based ONLY on the context provided above, please answer the following question.\n",
        "User Question: {query}\"\"\"\n",
        "        # print(f\"\\nContext prepared for LLM (using {len(retrieved_chunks_content)} chunk(s)).\")\n",
        "    else:\n",
        "        user_message_text = f\"\"\"User Question: {query}\n",
        "\n",
        "(System note: No specific context was retrieved for this question from the documents. \n",
        "Based on your instructions, if the answer is not in the documents, please state that.)\"\"\"\n",
        "\n",
        "    chat_history.add_user_message(user_message_text)\n",
        "\n",
        "    # --- 3. Generate an answer using the chat service's get_chat_message_contents ---\n",
        "    try:\n",
        "        execution_settings = OpenAIChatPromptExecutionSettings(\n",
        "            service_id = chat_service_id,\n",
        "        )\n",
        "        \n",
        "        response_messages = await chat_completion_service.get_chat_message_contents(\n",
        "            chat_history = chat_history,\n",
        "            settings = execution_settings\n",
        "        )\n",
        "\n",
        "        print(\"\\nLLM Response:\")\n",
        "        if response_messages and isinstance(response_messages, list) and len(response_messages) > 0:\n",
        "            assistant_message_content = response_messages[0]\n",
        "            if hasattr(assistant_message_content, 'content') and assistant_message_content.content is not None:\n",
        "                 print(str(assistant_message_content.content))\n",
        "            elif hasattr(assistant_message_content, 'items') and assistant_message_content.items and \\\n",
        "                 hasattr(assistant_message_content.items[0], 'text'):\n",
        "                 print(assistant_message_content.items[0].text)\n",
        "            else: \n",
        "                 print(str(assistant_message_content))\n",
        "        else:\n",
        "            print(\"\\nLLM returned an empty or unexpected response format.\")\n",
        "            print(f\"Raw response: {response_messages}\")\n",
        "\n",
        "    except AttributeError as e:\n",
        "        print(f\"AttributeError during LLM invocation: {e}\")\n",
        "        traceback.print_exc()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM invocation: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"RAG function is ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2423d0d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# General RAG query across all documents\n",
        "await perform_rag_query(\n",
        "    query=\"What are the key benefits of the Northwind Health Plus plan?\",\n",
        "    top_k_retrieval=3\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cleanup",
      "metadata": {},
      "source": [
        "### ðŸªœ Step 8: Cleanup (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cleanup_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up the vector collection\n",
        "async def cleanup_collection():\n",
        "    if vector_collection:\n",
        "        try:\n",
        "            async with vector_collection:\n",
        "                await vector_collection.delete_collection()\n",
        "            print(\"Vector collection cleaned up\")\n",
        "        except Exception as e:\n",
        "            print(f\"Cleanup error: {e}\")\n",
        "    else:\n",
        "        print(\"No collection to clean up\")\n",
        "\n",
        "await cleanup_collection()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
