{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00088d4",
   "metadata": {},
   "source": [
    "## Semantic Kernel: Ramp-Up based on SK's Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d481c",
   "metadata": {},
   "source": [
    "To get the latest version of SK Python package, use:\n",
    "\n",
    "``` bash\n",
    "pip install --upgrade semantic-kernel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db6c3f",
   "metadata": {},
   "source": [
    "## ðŸ“’ Notebook 1: Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e4fe7",
   "metadata": {},
   "source": [
    "### ðŸªœ Step 1: Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ba3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Annotated\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c91018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Azure OpenAI backend variables\n",
    "AOAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_API_DEPLOY\")\n",
    "AOAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
    "AOAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f1baa",
   "metadata": {},
   "source": [
    "### ðŸªœ Step 2: Add AI services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67498941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise kernel\n",
    "kernel = Kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a59c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Azure OpenAI chat completion\n",
    "chat_completion = AzureChatCompletion(\n",
    "    deployment_name = AOAI_DEPLOYMENT,\n",
    "    endpoint = AOAI_ENDPOINT,\n",
    "    api_version = AOAI_API_VERSION,\n",
    ")\n",
    "\n",
    "kernel.add_service(chat_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad19d331",
   "metadata": {},
   "source": [
    "### ðŸªœ Step 3: Enable logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d199dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging level for SK to DEBUG.\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "logging.getLogger(\"kernel\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a6a31",
   "metadata": {},
   "source": [
    "### ðŸªœ Step 4: Add plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27230268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lights plugin\n",
    "class LightsPlugin:\n",
    "    lights = [\n",
    "        {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False},\n",
    "        {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False},\n",
    "        {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True},\n",
    "    ]\n",
    "\n",
    "    @kernel_function(\n",
    "        name=\"get_lights\",\n",
    "        description=\"Gets a list of lights and their current state\",\n",
    "    )\n",
    "    def get_state(\n",
    "        self,\n",
    "    ) -> str:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self.lights\n",
    "\n",
    "    @kernel_function(\n",
    "        name=\"change_state\",\n",
    "        description=\"Changes the state of the light\",\n",
    "    )\n",
    "    def change_state(\n",
    "        self,\n",
    "        id: int,\n",
    "        is_on: bool,\n",
    "    ) -> str:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                light[\"is_on\"] = is_on\n",
    "                return light\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f65a23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Lights', description=None, functions={'change_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='change_state', plugin_name='Lights', description='Changes the state of the light', parameters=[KernelParameterMetadata(name='id', description=None, default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer'}, include_in_function_choices=True), KernelParameterMetadata(name='is_on', description=None, default_value=None, type_='bool', is_required=True, type_object=<class 'bool'>, schema_data={'type': 'boolean'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000021B6F8E8E10>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000021B7001E910>, method=<bound method LightsPlugin.change_state of <__main__.LightsPlugin object at 0x0000021B70033010>>, stream_method=None), 'get_lights': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_lights', plugin_name='Lights', description='Gets a list of lights and their current state', parameters=[], is_prompt=False, is_asynchronous=False, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000021B70028910>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000021B70028150>, method=<bound method LightsPlugin.get_state of <__main__.LightsPlugin object at 0x0000021B70033010>>, stream_method=None)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add plugin to the kernel\n",
    "kernel.add_plugin(\n",
    "    LightsPlugin(),\n",
    "    plugin_name=\"Lights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b308d4",
   "metadata": {},
   "source": [
    "### ðŸªœ Step 5: Define behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c649ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-invoke function calling\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f2671",
   "metadata": {},
   "source": [
    "### ðŸªœ Step 6: Run the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a1aafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conversation history\n",
    "history = ChatHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8355c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get AI response\n",
    "async def get_response(prompt):\n",
    "    # Add the prompt to the history\n",
    "    history.add_user_message(prompt)\n",
    "\n",
    "    # Get the response from the AI\n",
    "    result = (await chat_completion.get_chat_message_contents(\n",
    "        chat_history = history,\n",
    "        settings = execution_settings,\n",
    "        kernel = kernel,\n",
    "        arguments = KernelArguments(),\n",
    "    ))[0]\n",
    "\n",
    "    # Add the response to the history\n",
    "    history.add_message(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d352fc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Get the lights state\n",
      "AI: There are three lights with the following states:\n",
      "- Table Lamp: Off\n",
      "- Porch light: Off\n",
      "- Chandelier: On\n",
      "\n",
      "Would you like to change the state of any of these lights?\n",
      "\n",
      "User: Change the state of the table lamp to on\n",
      "AI: The Table Lamp has been turned on. Is there anything else you'd like to do?\n",
      "\n",
      "User: Get the lights state\n",
      "AI: The current state of the lights is:\n",
      "- Table Lamp: On\n",
      "- Porch light: Off\n",
      "- Chandelier: On\n",
      "\n",
      "Let me know if you want to change anything or need further assistance.\n",
      "\n",
      "User: Change the state of the porch light to on\n",
      "AI: The Porch light has been turned on. If you need any more changes or information, feel free to ask!\n",
      "\n",
      "User: Get the lights state\n",
      "AI: All the lights are currently on:\n",
      "- Table Lamp: On\n",
      "- Porch light: On\n",
      "- Chandelier: On\n",
      "\n",
      "Is there anything else you would like to do?\n",
      "\n",
      "User: Change the state of the chandelier to off\n",
      "AI: The Chandelier has been turned off. If you need any further assistance, please let me know!\n",
      "\n",
      "User: Get the lights state\n",
      "AI: The current state of the lights is:\n",
      "- Table Lamp: On\n",
      "- Porch light: On\n",
      "- Chandelier: Off\n",
      "\n",
      "Let me know if you need any more changes or information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test AI function calling\n",
    "queries = [\n",
    "    \"Get the lights state\",\n",
    "    \"Change the state of the table lamp to on\",\n",
    "    \"Get the lights state\",\n",
    "    \"Change the state of the porch light to on\",\n",
    "    \"Get the lights state\",\n",
    "    \"Change the state of the chandelier to off\",\n",
    "    \"Get the lights state\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"User: {query}\")\n",
    "    response = await get_response(query)\n",
    "    print(f\"AI: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
